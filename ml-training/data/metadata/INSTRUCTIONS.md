# 📊 Dataset Metadata Instructions

## 📁 Expected Files

This directory should contain metadata files generated during data processing:

```
metadata/
├── dataset_info.json          # Overall dataset statistics
├── class_distribution.json    # Per-class sample counts
├── image_quality_metrics.csv  # Quality assessment results
├── data_splits.json          # Train/validation/test splits
├── preprocessing_log.txt      # Processing pipeline log
├── collection_metadata.csv    # Image collection information
└── annotations.csv           # Image annotations and labels
```

## 📋 File Descriptions

### **dataset_info.json**
Overall dataset statistics and configuration:
```json
{
  "total_images": 10000,
  "num_classes": 7,
  "class_names": ["healthy", "bacterial_wilt", "rhizome_rot", "leaf_spot", 
                  "soft_rot", "yellow_disease", "root_knot_nematode"],
  "creation_date": "2024-01-15",
  "version": "1.0.0",
  "preprocessing_config": {
    "img_height": 224,
    "img_width": 224,
    "validation_split": 0.15,
    "test_split": 0.15
  }
}
```

### **class_distribution.json**
Sample counts per disease class:
```json
{
  "healthy": {"train": 1400, "validation": 300, "test": 300},
  "bacterial_wilt": {"train": 1050, "validation": 225, "test": 225},
  "rhizome_rot": {"train": 840, "validation": 180, "test": 180},
  "leaf_spot": {"train": 1050, "validation": 225, "test": 225},
  "soft_rot": {"train": 700, "validation": 150, "test": 150},
  "yellow_disease": {"train": 560, "validation": 120, "test": 120},
  "root_knot_nematode": {"train": 700, "validation": 150, "test": 150}
}
```

### **image_quality_metrics.csv**
Quality assessment for each image:
```csv
filename,resolution,blur_score,brightness,contrast,quality_score
healthy_001.jpg,512x512,45.2,128.5,42.1,0.85
bacterial_wilt_002.jpg,480x640,38.7,115.2,38.9,0.78
```

### **data_splits.json**
Train/validation/test split assignments:
```json
{
  "train": ["healthy_001_20240115.jpg", "bacterial_wilt_002_20240115.jpg", ...],
  "validation": ["healthy_050_20240116.jpg", "leaf_spot_051_20240116.jpg", ...],
  "test": ["rhizome_rot_100_20240117.jpg", "soft_rot_101_20240117.jpg", ...]
}
```

### **preprocessing_log.txt**
Processing pipeline execution log:
```
2024-01-15 10:30:00 - Starting data preprocessing
2024-01-15 10:30:05 - Found 10,234 raw images
2024-01-15 10:30:10 - Quality filtering: removed 234 low-quality images
2024-01-15 10:31:00 - Resizing and normalization complete
2024-01-15 10:31:30 - Data splitting complete
2024-01-15 10:31:35 - Processing finished successfully
```

### **collection_metadata.csv**
Image collection information:
```csv
filename,collection_date,location,collector,device,notes
healthy_001.jpg,2024-01-10,Farm_A,John_Doe,iPhone_13,Morning_light
bacterial_wilt_002.jpg,2024-01-11,Farm_B,Jane_Smith,Samsung_S21,Severe_symptoms
```

### **annotations.csv**
Image labels and additional annotations:
```csv
filename,class_label,class_id,confidence,severity,plant_part,growth_stage
healthy_001.jpg,healthy,0,1.0,none,whole_plant,mature
bacterial_wilt_002.jpg,bacterial_wilt,1,0.95,severe,stems,vegetative
```

## 🔄 Automatic Generation

These files are automatically generated by:

### **Data Preprocessing Pipeline**
```bash
python data_preprocessing.py
```
Generates:
- `dataset_info.json`
- `class_distribution.json`
- `data_splits.json`
- `preprocessing_log.txt`

### **Quality Assessment Script**
```bash
python assess_data_quality.py
```
Generates:
- `image_quality_metrics.csv`

### **Collection Import Script**
```bash
python import_collection_metadata.py
```
Generates:
- `collection_metadata.csv`
- `annotations.csv`

## 📊 Usage Examples

### **Loading Dataset Info**
```python
import json

with open('metadata/dataset_info.json', 'r') as f:
    dataset_info = json.load(f)
    
print(f"Total images: {dataset_info['total_images']}")
print(f"Classes: {dataset_info['class_names']}")
```

### **Checking Class Distribution**
```python
import json
import pandas as pd

with open('metadata/class_distribution.json', 'r') as f:
    distribution = json.load(f)

# Convert to DataFrame for analysis
df = pd.DataFrame(distribution).T
print(df)
```

### **Quality Analysis**
```python
import pandas as pd

quality_df = pd.read_csv('metadata/image_quality_metrics.csv')
print(f"Average quality score: {quality_df['quality_score'].mean():.3f}")
print(f"Low quality images: {(quality_df['quality_score'] < 0.7).sum()}")
```

## 🔍 Validation

### **Metadata Consistency Check**
```bash
python validate_metadata.py
```

Validates:
- File counts match across metadata files
- Class labels are consistent
- No missing or corrupted entries
- Data splits don't overlap

### **Manual Verification**
- [ ] All expected files are present
- [ ] JSON files are valid format
- [ ] CSV files have proper headers
- [ ] File counts match actual images
- [ ] No duplicate entries

## 📝 Maintenance

### **Updates**
When adding new images:
1. Place in appropriate `raw/` directory
2. Run preprocessing pipeline
3. Metadata files will be automatically updated
4. Commit updated metadata to version control

### **Version Control**
- **Include in Git**: All metadata files (they're small)
- **Track changes**: Monitor metadata diffs for dataset evolution
- **Backup**: Keep copies of metadata for dataset versions

## 🚨 Important Notes

- **Do not manually edit** generated metadata files
- **Regenerate** metadata when raw data changes
- **Backup** metadata before major preprocessing changes
- **Validate** metadata consistency regularly
